{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0N8O29sZtvM"
      },
      "source": [
        "# Lab 1: Introduction to Generative AI\n",
        "\n",
        "The lab is divided in two parts.\n",
        "In the first part, we will introduce simple generative models for a discrete and a continuous distribution.\n",
        "The goal is to present the basics of training a probabilitic model.\n",
        "\n",
        "In the second part, we study the generation of sentences with large languages models (LLMs).\n",
        "Most LLMs are auto-regressive models which iteratively produce a discrete distribution of the next word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7JkvL-OZtvP"
      },
      "source": [
        "## 0. Preconfiguration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuJUSaxgZtvP"
      },
      "source": [
        "We install the required libaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOlI1qwhZtvP"
      },
      "outputs": [],
      "source": [
        "%pip install --quiet --upgrade transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-d4yGPSZtvQ"
      },
      "source": [
        "We import useful libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jy_FsXpJZtvQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.distributions import Normal, Categorical, MixtureSameFamily, Independent\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_wIqMZxZtvQ"
      },
      "source": [
        "We check that a GPU is available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAc3VNleZtvR"
      },
      "outputs": [],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6k_YlcLIZtvR"
      },
      "source": [
        "In Google Colab, if the above code returns False, click on `Runtime`, `Change runtime type`, and choose `T4 GPU`. Then run the notebook again from the start."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWYNidLaZtvR"
      },
      "source": [
        "## 1. A simple generative model for a discrete distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rd5F1xOkZtvR"
      },
      "source": [
        "We consider a random variable $X$ with a categorical distribution over 3 classes $C_1$, $C_2$ and $C_3$.\n",
        "The probabilities for each classes are $\\mathbb{P}(X = C_1) = 0.1$, $\\mathbb{P}(X = C_2) = 0.1$ and $\\mathbb{P}(X = C_3) = 0.8$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1yd4E0MZtvT"
      },
      "outputs": [],
      "source": [
        "true_probabilities = [0.1, 0.1, 0.8]\n",
        "true_dist = Categorical(torch.tensor(true_probabilities))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNBqLTnLZtvU"
      },
      "source": [
        "The probability simplex over three classes is a triangular representation used to visualize probability distributions.\n",
        "Any point inside the triangle represents a probability distribution among the three classes. The closer a point is to a particular vertex, the higher the probability of that corresponding class.\n",
        "A point exactly in the center would indicate equal probabilities for all three classes.\n",
        "In the following plot, we indicate the true probability in the probability simplex."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VxLb-DTXbw69"
      },
      "outputs": [],
      "source": [
        "#@title Simplex plotting function\n",
        "\n",
        "import matplotlib.tri as tri\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def plot_simplex(probas=None, true_probas=None):\n",
        "    # Create a triangular grid\n",
        "    corners = np.array([[0, 0], [1, 0], [0.5, np.sqrt(3)/2]])\n",
        "    triangle = tri.Triangulation(corners[:, 0], corners[:, 1], [[0, 1, 2]])\n",
        "\n",
        "    # Create the plot\n",
        "    fig, axis = plt.subplots(figsize=(8, 7))\n",
        "    axis.triplot(triangle, 'k-')\n",
        "\n",
        "    # Label the vertices\n",
        "    alignements = [('right', 'top'), ('left', 'top'), ('center', 'bottom')]\n",
        "    labels = ['$C_3$', '$C_1$', '$C_2$']\n",
        "    for i, corner in enumerate(corners):\n",
        "        ha, va = alignements[i]\n",
        "        axis.text(corner[0], corner[1], labels[i], ha=ha, va=va)\n",
        "\n",
        "    if probas is not None:\n",
        "        probas = np.array(probas)\n",
        "        x = probas[:, 0] + probas[:, 1]/2\n",
        "        y = probas[:, 1] * np.sqrt(3)/2\n",
        "        plt.plot(x, y, 'o', markersize=5)\n",
        "\n",
        "    if true_probas is not None:\n",
        "        x = true_probas[0] + true_probas[1]/2\n",
        "        y = true_probas[1] * np.sqrt(3)/2\n",
        "        plt.plot(x, y, 'ro', markersize=5)\n",
        "\n",
        "    # Set plot limits\n",
        "    axis.set_xlim(-0.1, 1.1)\n",
        "    axis.set_ylim(-0.1, 1.1 * np.sqrt(3)/2)\n",
        "\n",
        "    axis.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v73cGqbJZtvU"
      },
      "outputs": [],
      "source": [
        "plot_simplex(true_probas=true_probabilities)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BqHAxaFZtvU"
      },
      "source": [
        "We suppose that we have access to a dataset of 1000 samples of $X$ and we show the first 20 instances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DYwvIPJZtvU"
      },
      "outputs": [],
      "source": [
        "samples = true_dist.sample((1000,))\n",
        "samples[:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mY3Rmu5TZtvU"
      },
      "source": [
        "We can estimate the distribution of $X$ by counting the number of samples in each class with a histogram."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J93SDI6pZtvU"
      },
      "outputs": [],
      "source": [
        "plt.bar(['$C_1$', '$C_2$', '$C_3$'], np.bincount(samples))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ru8fRxkZtvU"
      },
      "source": [
        "In future labs, we will study the training procedure of Deep Generative Models.\n",
        "To have a better understanding, we will estimate the distribution of $X$ with the same optimization procedure than Deep Generative Models: **maximum likelihood estimation** with **gradient descent**.\n",
        "We define a PyTorch module for a categorical distribution with one parameter per class.\n",
        "We use the softmax function to ensure that the probabilities sum to 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feUgh1cvZtvU"
      },
      "outputs": [],
      "source": [
        "class CategoricalModule(torch.nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super().__init__()\n",
        "        self.logits = torch.nn.Parameter(torch.zeros(n_classes))\n",
        "\n",
        "    def dist(self):\n",
        "        probs = torch.softmax(self.logits, dim=-1)\n",
        "        return Categorical(probs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6QMp7vsZtvU"
      },
      "source": [
        "We can access the probabilities of our distribution using `.probs`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBpMh1leZtvU"
      },
      "outputs": [],
      "source": [
        "module = CategoricalModule(3)\n",
        "module.dist().probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOc062aBZtvV"
      },
      "source": [
        "We can also sample using `.sample()` and evaluate the probability of a sample using `.log_prob()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NymZp700ZtvV"
      },
      "outputs": [],
      "source": [
        "sample = module.dist().sample()\n",
        "print(sample)\n",
        "print(module.dist().log_prob(sample).exp())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vq36A8gqZtvV"
      },
      "source": [
        "In this code, we perform maximum likelihood estimation using gradient descent with the optimizer Adam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLeAH7ZhZtvV"
      },
      "outputs": [],
      "source": [
        "module = CategoricalModule(3)\n",
        "loss_list = []\n",
        "probs_list = []\n",
        "\n",
        "lr = 0.01\n",
        "nb_steps = 300\n",
        "optimizer = torch.optim.Adam(module.parameters(), lr=lr)\n",
        "for step in range(nb_steps):\n",
        "    optimizer.zero_grad()\n",
        "    dist = module.dist()\n",
        "    # We compute the negative log-likelihood of the samples\n",
        "    loss = -dist.log_prob(samples).mean()\n",
        "    # We compute the gradients\n",
        "    loss.backward()\n",
        "    # We modify the parameters using gradient descent\n",
        "    optimizer.step()\n",
        "    loss_list.append(loss.item())\n",
        "    # We collect the probabilities every 20 steps\n",
        "    if step % 20 == 0:\n",
        "        probs = dist.probs.detach().numpy()\n",
        "        probs_list.append(probs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MUU7rxjZtvV"
      },
      "source": [
        "We plot the probability simplex with the estimated probabilities per step in blue and the true probability in red."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCP1CCQmZtvV"
      },
      "outputs": [],
      "source": [
        "plot_simplex(probas=probs_list, true_probas=true_probabilities)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkWnlrxXZtvV"
      },
      "source": [
        "Can you plot the loss per step?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRTmmoB-ZtvV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpcAtYOPZtvV"
      },
      "source": [
        "You can experiment with other learning rates `lr` and number of steps `nb_steps`. What do you observe?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZOSiJaiZtvV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dz4U34ecZtvV"
      },
      "source": [
        "Suppose that we change the true probabilities to the probabilities defined below.\n",
        "\n",
        "Can you execute the optimization algorithm with these true probabilities?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xufZaOYhZtvW"
      },
      "outputs": [],
      "source": [
        "true_probabilities = [0.45, 0.45, 0.1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLQhqmlEZtvW"
      },
      "source": [
        "## 2. A simple generative model for a continuous distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shdQGmcPZtvW"
      },
      "source": [
        "We now consider a generative model for a two-dimensional continuous variable $X \\in \\mathbb{R}^2$ from the dataset *moons*. We have access to a datasets of 1000 samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTKSwpmIZtvW"
      },
      "outputs": [],
      "source": [
        "X, _ = make_moons(n_samples=1000, noise=0.1)\n",
        "X = torch.from_numpy(X).float()\n",
        "plt.scatter(X[:, 0], X[:, 1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNYIBQqrZtvW"
      },
      "source": [
        "We define a PyTorch module for a Gaussian mixture model where the mean, standard deviation and weight of each component are estimated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkWdmBpXZtvW"
      },
      "outputs": [],
      "source": [
        "class MixtureModule(torch.nn.Module):\n",
        "    def __init__(self, n_components, d):\n",
        "        super().__init__()\n",
        "        self.mean = torch.nn.Parameter(torch.randn((n_components, d)))\n",
        "        self.rho = torch.nn.Parameter(torch.ones_like(self.mean))\n",
        "        self.logits = torch.nn.Parameter(torch.zeros(n_components))\n",
        "\n",
        "    def dist(self):\n",
        "        probs = torch.softmax(self.logits, dim=-1)\n",
        "        stddev = torch.nn.Softplus()(self.rho) + 1e-5\n",
        "        return MixtureSameFamily(Categorical(probs), Independent(Normal(self.mean, stddev), 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvdD1fQ0ZtvZ"
      },
      "source": [
        "We can access the means, standard deviations and weights using the following code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90oK-3kwZtva"
      },
      "outputs": [],
      "source": [
        "module = MixtureModule(3, 2) # 3 components, 2 dimensions\n",
        "dist = module.dist()\n",
        "print(dist.component_distribution.mean)\n",
        "print(dist.component_distribution.stddev)\n",
        "print(dist.mixture_distribution.probs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvb0ZA8oZtva"
      },
      "source": [
        "As for the categorical distribution, we can also sample using `.sample()` and evaluate the probability of a sample using `.log_prob()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86WMFgztZtva"
      },
      "outputs": [],
      "source": [
        "sample = module.dist().sample()\n",
        "print(sample)\n",
        "print(module.dist().log_prob(sample).exp())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScdSpleTZtva"
      },
      "source": [
        "Can you plot 1000 samples?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCdPRGdCZtva"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rc-4D40UZtva"
      },
      "source": [
        "As before, we estimate the parameters using maximum likelihood estimation with gradient descent.\n",
        "\n",
        "Modify the code to:\n",
        "1. Collect the loss at each step and plot the loss per step at the end of training.\n",
        "2. Plot samples of the estimated distribution after steps 0, 500, 1000, ..., 3500.\n",
        "3. On the same plot, add the mean of each component of the mixture.\n",
        "\n",
        "You can take inspiration from the training code for the discrete distribution (Section 1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCTUNTZ1Ztva"
      },
      "outputs": [],
      "source": [
        "module = MixtureModule(30, 2) # 30 components, 2 dimensions\n",
        "\n",
        "lr = 0.004\n",
        "nb_steps = 3500\n",
        "optimizer = torch.optim.Adam(module.parameters(), lr=lr)\n",
        "for step in range(nb_steps):\n",
        "    optimizer.zero_grad()\n",
        "    dist = module.dist()\n",
        "    loss = -dist.log_prob(X).mean()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6Um8iEVZtva"
      },
      "source": [
        "Experiment with other learning rates `lr` and number of steps `nb_steps`. What do you observe?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fqo-wVEZtva"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whzl9hLiZtva"
      },
      "source": [
        "One limitation of generative models is that they can learn to copy data. This phenomenon is known as overfitting.\n",
        "This will be especially the case if there is less data than parameters.\n",
        "\n",
        "In the previous experiment, we used a dataset of size 1000. Can you create a dataset of size 10, run the training algorithm, and note your observations?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qe6nj7fUZtva"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jjagCx7Ztva"
      },
      "source": [
        "## 3. Large language models (LLMs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Piv3VZ63Ztva"
      },
      "source": [
        "We consider the model GPT-2 (Generative Pre-trained Transformer 2), which has been developed by OpenAI in 2019.\n",
        "\n",
        "It has been trained on BookCorpus, a dataset of over 7,000 self-published fiction books from various genres, and trained on a dataset of 8 million web pages.\n",
        "\n",
        "In contrast to GPT-3 and GPT-4, it is an open-source models, which means that its parameters have been released to the public.\n",
        "\n",
        "We first download and load the parameters of the large version of the model (774 million parameters)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8j1NpaiJZtvb"
      },
      "outputs": [],
      "source": [
        "#model_name = \"llmware/bling-1b-0.1\"\n",
        "model_name = \"gpt2-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkAPQ-axZtvb"
      },
      "source": [
        "We count the total number of parameters by summing the number of elements in each group of parameters `p`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKy4fnD-Ztvb"
      },
      "outputs": [],
      "source": [
        "sum(p.numel() for p in model.parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oA1KEbg5Ztvb"
      },
      "source": [
        "First, we encode an input text to a list of numbers using a tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjxV0XJxZtvb"
      },
      "outputs": [],
      "source": [
        "input_text = \"My favorite food is\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "input_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFY1Uq7VZtvb"
      },
      "source": [
        "By calling the model, we get *logits*, which are then transformed into probabilities using the *softmax* function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QG3kn1_Ztvb"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids, labels=input_ids)\n",
        "logits = outputs.logits\n",
        "# We take the probabilities of the last token (the one we want to predict)\n",
        "next_token_logits = logits[0, -1, :]\n",
        "probabilities = torch.softmax(next_token_logits, dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueuZFFLPZtvb"
      },
      "source": [
        "For illustration, we show the probabilities assigned to the 10 most likely classes and the 10 less likely classes.\n",
        "\n",
        "The first plot is in linear scale, while the second plot is in log scale so that the smallest probabilities are visible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-x3vO7XZtvb"
      },
      "outputs": [],
      "source": [
        "# We get the sorted probabilities and IDs\n",
        "tokens_probas, tokens_ids = probabilities.sort()\n",
        "# We select the top 10 and bottom 10 tokens\n",
        "tokens_ids = torch.cat([tokens_ids[:10], tokens_ids[-10:]])\n",
        "tokens_probas = torch.cat([tokens_probas[:10], tokens_probas[-10:]])\n",
        "# We convert the tokens indices to strings\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokens_ids)\n",
        "# Horizontal bar plot, first with linear scale, then with log scale\n",
        "for log_scale in [False, True]:\n",
        "    fig, axis = plt.subplots(figsize=(5, 6))\n",
        "    axis.barh(tokens, tokens_probas, orientation='horizontal')\n",
        "    if log_scale:\n",
        "        axis.set_xscale('log')\n",
        "        axis.set_xlabel('Probability')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXoaCrqZZtvb"
      },
      "source": [
        "In the above example, we observe that there is high uncertainty on the next word in the sentence.\n",
        "\n",
        "Can you think of a sentence `input_text` for which there would be low uncertainty on the next word? Then, check that this is indeed the case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJi1DkvmZtvb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73dtR5nlZtvb"
      },
      "source": [
        "As in Section 1, we can create a categorical distribution of the next word and sample from it. Then, we can decode the token ID."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cXe9B5OZtvc"
      },
      "outputs": [],
      "source": [
        "dist = Categorical(probs=probabilities)\n",
        "next_token_id = dist.sample()\n",
        "tokenizer.decode(next_token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m07HZdJEZtvc"
      },
      "source": [
        "Finally, we generate a full sentence one word at a time using an auto-regressive sampling procedure.\n",
        "\n",
        "We use a more recent model for better results. Note that this model is still relatively small (1 billion parameters) compared to recent LLMs (e.g., 175 billion parameters for GPT-3)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAIhY_qdZtvc"
      },
      "outputs": [],
      "source": [
        "def generate_sentence_with_probabilities(model, tokenizer, initial_text, temperature=1., max_length=20):\n",
        "    input_ids = tokenizer.encode(initial_text, return_tensors='pt')\n",
        "    model.eval()\n",
        "\n",
        "    sentence = initial_text\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids)\n",
        "        logits = outputs.logits\n",
        "        # (Section 4): Implement the temperature here\n",
        "        next_token_logits = logits[0, -1, :]\n",
        "        probabilities = torch.softmax(next_token_logits, dim=-1)\n",
        "        # Choose the most probable next token\n",
        "        dist = Categorical(probs=probabilities)\n",
        "        next_token_id = dist.sample(torch.Size([1]))\n",
        "        # Append the token to the input_ids\n",
        "        input_ids = torch.cat([input_ids, next_token_id.unsqueeze(0)], dim=1)\n",
        "        # Decode the token to append it to the sentence\n",
        "        next_word = tokenizer.decode(next_token_id)\n",
        "        print(f'{next_word}')\n",
        "\n",
        "        sentence += next_word\n",
        "        if next_word == tokenizer.eos_token:\n",
        "            break\n",
        "    return sentence\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "model_name = \"llmware/bling-1b-0.1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Generate a sentence\n",
        "initial_text = \"I like playing with large language models because\"\n",
        "\n",
        "sentence = generate_sentence_with_probabilities(model, tokenizer, initial_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkIlNC30Ztvc"
      },
      "outputs": [],
      "source": [
        "print(\"Generated Sentence:\", sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZC2M8RLKZtvc"
      },
      "source": [
        "Can you display the probability of the sampled word at each step?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTzxcvsfZtvc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ywq1RZ61Ztvc"
      },
      "source": [
        "Can you modify the above code to predict the most probable word at each step instead of sampling from a categorical distribution?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-lpGMBRZtvc"
      },
      "source": [
        "## 4. Optional experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntOjeXohZtvc"
      },
      "source": [
        "We can increase or decrease the uncertainty of the model with a hyperparameter $t$ called the **temperature** which influences the level of creativity in the responses.\n",
        "\n",
        "In practice, we can implement this easily by dividing the logits by $t$: `logits = logits / t`.\n",
        "\n",
        "When set high, it produces more inventive and imaginative text. Conversely, a lower temperature yields responses that are more precise and factual. A temperature of 1 will have no effect.\n",
        "\n",
        "Modify the above code to a temperature parameter. Then, you can try very values of temperature such as `1e-5`, which will produce more factual outputs and `1.2`, which will produce more creative outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnHVM95_Ztvc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IJnY8XrZtvc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZkiG_3_Ztvc"
      },
      "source": [
        "Larger languages models require more GPU memory than what is available in Google Colab. If you have a GPU with sufficient memory, you can experiment with more recent LLMs such as Llama-2. These models are open-source at the condition of accepting license terms (see [here](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)), which can take a few minutes to 2 days.\n",
        "You can then generate an authentication token [here](https://huggingface.co/settings/tokens) and run the command below with your authentication token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xjh_Eq1IZtvc"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login --token <YOUR_TOKEN>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxuRPbWTZtvc"
      },
      "source": [
        "The model can then be instantiated with this code in a similar way than the models from Section 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3sQz-cXZtvc"
      },
      "outputs": [],
      "source": [
        "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
        "\n",
        "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "model = LlamaForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = LlamaTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYAYl6HqZtvc"
      },
      "outputs": [],
      "source": [
        "sentence, token_probs = generate_sentence_with_probabilities(model, tokenizer, initial_text, temperature=1.5)\n",
        "print(\"Generated Sentence:\", sentence)\n",
        "print(\"Token Probabilities:\", token_probs)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
